{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu00608/miniconda3/envs/lofi/lib/python3.6/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, \\\n",
    "    Lambda, Reshape, Conv2DTranspose, Layer, InputLayer, Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import binary_crossentropy, kl_divergence\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 64\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\", 'r'))\n",
    "params = config[\"params\"]\n",
    "encoder_params = config[\"encoder\"]\n",
    "decoder_params = config[\"decoder\"]\n",
    "\n",
    "def show_random_data(data_list, label_list, pick=None):\n",
    "    if not pick:\n",
    "        pick = random.randint(0, len(data_list))\n",
    "    plt.title(f\"Label : {label_list[pick]}\")\n",
    "    plt.imshow(data_list[pick], 'gray')\n",
    "    return pick\n",
    "\n",
    "def sample(inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    # dim = tf.shape(z_mean)[0]\n",
    "    eps = tf.random.normal(shape=(1, params[\"z_dim\"]))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (mu, log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (mu, log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, params, encoder_input_shape, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.conv = []\n",
    "        for n_layer in range(len(params[\"conv_filters\"])):\n",
    "\n",
    "            self.conv.append(Conv2D(\n",
    "                    params[\"conv_filters\"][n_layer], \n",
    "                    params[\"conv_kernels\"][n_layer],\n",
    "                    (params[\"conv_strides\"][n_layer], params[\"conv_strides\"][n_layer]), \n",
    "                    padding='same' \n",
    "                )\n",
    "            )\n",
    "        self.relu = Activation(\"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.mu = Dense(params[\"z_dim\"])\n",
    "        self.log_var = Dense(params[\"z_dim\"])\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for n_layer in range(len(params[\"conv_filters\"])):\n",
    "            x = self.conv[n_layer](x)\n",
    "            x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.sampling((mu, log_var))\n",
    "        return mu, log_var, z\n",
    "\n",
    "class Decoder(Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, params, decoder_input_shape, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.linear = Dense(np.prod((14, 14, 32)), activation=\"relu\")\n",
    "        self.reshape = Reshape(target_shape=(14, 14, 32))\n",
    "        self.conv_t = []\n",
    "        for n_layer in range(len(params[\"conv_t_filters\"])):\n",
    "            self.conv_t.append(\n",
    "                Conv2DTranspose(\n",
    "                    params[\"conv_t_filters\"][n_layer], \n",
    "                    params[\"conv_t_kernels\"][n_layer],\n",
    "                    (params[\"conv_t_strides\"][n_layer], params[\"conv_t_strides\"][n_layer]), \n",
    "                    padding='same' \n",
    "                )\n",
    "            )\n",
    "        self.relu = Activation(\"relu\")\n",
    "        self.sigmoid = Activation(\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear(inputs)\n",
    "        x = self.reshape(x)\n",
    "        for n_layer in range(len(params[\"conv_filters\"])):\n",
    "            x = self.conv_t[n_layer](x)\n",
    "            if n_layer < len(params[\"conv_t_filters\"]) - 1:\n",
    "                x = self.relu(x)\n",
    "            else:\n",
    "                x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_input_shape,\n",
    "        params,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(params, encoder_input_shape)\n",
    "        self.decoder = Decoder(params, (params[\"z_dim\"]))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "def plot_latent_images(name, n=20, digit_size=28, additional_msg=''):\n",
    "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    image_width = digit_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = np.array([[xi], [yi]])\n",
    "            z = sample(z)\n",
    "            x_decoded = vae.decoder(z)\n",
    "            digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
    "            image[i * digit_size: (i + 1) * digit_size,\n",
    "                    j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.title(f'{epochs} epochs, {latent_dim} Latent Spaces, {batch_size} Batch Size. '+additional_msg)\n",
    "    plt.savefig('out/'+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = np.expand_dims(x_train, len(x_train.shape))\n",
    "x_test = np.expand_dims(x_test, len(x_test.shape))\n",
    "\n",
    "# (width, height, channel)\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "original_dim = np.prod(input_shape)\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
    "                 .shuffle(1024).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
    "                .shuffle(1024).batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update(encoder_params)\n",
    "params.update(decoder_params)\n",
    "\n",
    "vae = VariationalAutoEncoder(input_shape, params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "bc_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_metric = tf.keras.metrics.Mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5\n",
      "935/938 [============================>.] - ETA: 0s - r_loss: 0.2170 - kl_loss: 0.0336 - loss: 0.2507\n",
      "\n",
      "Epoch 1/5\n",
      "937/938 [============================>.] - ETA: 0s - r_loss: 0.2001 - kl_loss: 0.0337 - loss: 0.2339\n",
      "\n",
      "Epoch 2/5\n",
      "934/938 [============================>.] - ETA: 0s - r_loss: 0.2207 - kl_loss: 0.0330 - loss: 0.2537\n",
      "\n",
      "Epoch 3/5\n",
      "934/938 [============================>.] - ETA: 0s - r_loss: 0.2229 - kl_loss: 0.0332 - loss: 0.2561\n",
      "\n",
      "Epoch 4/5\n",
      "935/938 [============================>.] - ETA: 0s - r_loss: 0.2124 - kl_loss: 0.0372 - loss: 0.2496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"r_loss\", \"kl_loss\", \"loss\"]\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    tf.print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    progbar = tf.keras.utils.Progbar(len(train_dataset), stateful_metrics=metrics)\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # print(reconstructed.shape)\n",
    "            # Compute reconstruction loss\n",
    "            r_loss = bc_loss_fn(x_batch_train, reconstructed)\n",
    "            # tf.print(vae.losses)\n",
    "            # break\n",
    "            total_loss = r_loss + sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        # r_loss, kl_loss, total_loss = train_step(x_batch_train)\n",
    "\n",
    "        loss_metric(total_loss)\n",
    "\n",
    "        progress_values = [(\"r_loss\", r_loss), (\"kl_loss\", sum(vae.losses)), (\"loss\", total_loss)]\n",
    "        progbar.update(step, values=progress_values)\n",
    "\n",
    "    tf.print(\"\\n\")\n",
    "\n",
    "        # if step % 100 == 0:\n",
    "        #     plot_latent_images(f\"Exp2-epoch{epoch}-step{step}-{loss_metric.result()}\")\n",
    "        #     print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_images(f'{latent_dim}-{batch_size}-{epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('lofi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af42d6b00596c858df57cfb1e3d56019ad36764fdda4c39cb12fa72d6aaee7c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
